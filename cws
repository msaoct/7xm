Cloud Computing:
Cloud computing is a virtualization-based technology that allows us to create, configure, and customize applications via an internet connection. The cloud technology includes a development platform, hard disk, software application, and database.
It is a technology that uses remote servers on the internet to store, manage, and access data online rather than local drives. The data can be anything such as files, images, documents, audio, video, and more.

Uses of Cloud Computing:
There are the following operations that we can do using cloud computing:
-Developing new applications and services.
-Storage, back up, and recovery of data.
-Hosting blogs and websites.
-Delivery of software on demand.
-Analysis of data.
-Streaming videos and audios.
-Social networking, email, hosting services & backup

Some well-known uses of cloud-computing are:
• Social Networking: Usually people do not realise that it is a use case of cloud computing but it is one of the most widely used example of cloud computing. Facebook, Twitter, LinkedIn and MySpace are some such examples. Social networking can be used not only for connecting with people online, but also for public relations (PR), business purposes and marketing. For example, a fan page is a popular PR practice these days.
• E-Mail: One of the prevalent cloud computing services is the Web-based e-mail. The cloud computing based e-mail service is useful in hosting an e-mail server and managing it. It also safeguards the e-mail server from being hacked. It also implies that your e-mail can be accessed from any place. Popular example of cloud computing based e-mail solution is Hotmail.
• Hosting Services: Cloud hosting services offer hosting for Websites on virtual servers that derive their computing resources from widespread underlying networks of physical Web servers. It is available as a service and not as a product. Therefore, the clients can utilise as much of the service as they need, based on the requirements of their Website and need to only pay for how much they use. Services such as online creation and storage of documents, spreadsheets, photographs and videos are examples of cloud based hosting services. Google docs, Onit, Google's Picasa, Yahoo's Flicker, YouTube, MetCafe and Vimeo offer such hosting and sharing services.
• Backup: A major issue with personal computing is the tendency of losing the data in case your computer is crashed, stolen, or the storage device is damaged. Therefore, the backup becomes essential. However, creating backups on media like CDs, Hard disk, etc. is not very effective. You should store the data off-site to have more comprehensive protection. Services such as, Mozy, Carbonite and JungleDisk, help in automatically backing up all the data to servers spread across the globe at a very low cost. Services such as, Google Drive, OneDrive, Dropbox and Syncplicity allow you to maintain local copies of data on multiple machines. All these files and data are synchronized and a copy is maintained in the cloud.

Characteristics of Cloud Computing:
The characteristics of cloud computing are given below:
1) Agility: The cloud works in a distributed computing environment. It shares resources among users and works very fast.
2) High availability and reliability: The availability of servers is high and more reliable because the chances of infrastructure failure are minimum.
3) High Scalability: Cloud offers "on-demand" provisioning of resources on a large scale, without having engineers for peak loads.
4) Multi-Sharing: With the help of cloud computing, multiple users and applications can work more efficiently with cost reductions by sharing common infrastructure.
5) Device and Location Independence: Cloud computing enables the users to access systems using a web browser regardless of their location or what device they use e.g. PC, mobile phone, etc. As infrastructure is off-site (typically provided by a third-party) and accessed via the Internet, users can connect from anywhere.
6) Maintenance: Maintenance of cloud computing applications is easier, since they do not need to be installed on each user's computer and can be accessed from different places. So, it reduces the cost also.
7) Low Cost: By using cloud computing, the cost will be reduced because to take the services of cloud computing, IT company need not to set its own infrastructure and pay-as-per usage of resources.
8) Services in the pay-per-use mode: Application Programming Interfaces (APIs) are provided to the users so that they can access services on the cloud by using these APIs and pay the charges as per the usage of services.

Advantages of Cloud Computing:
1) Back-up and restore data: Once the data is stored in the cloud, it is easier to get back-up and restore that data using the cloud.
2) Improved collaboration: Cloud applications improve collaboration by allowing groups of people to quickly and easily share information in the cloud via shared storage.
3) Excellent accessibility: Cloud allows us to quickly and easily access store information anywhere, anytime in the whole world, using an internet connection. An internet cloud infrastructure increases organization productivity and efficiency by ensuring that our data is always accessible.
4) Low maintenance cost: Cloud computing reduces both hardware and software maintenance costs for organizations.
5) Mobility: Cloud computing allows us to easily access all cloud data via mobile.
6) IServices in the pay-per-use model: Cloud computing offers Application Programming Interfaces (APIs) to the users for access services on the cloud and pays the charges as per the usage of service.
7) Unlimited storage capacity: Cloud offers us a huge amount of storing capacity for storing our important data such as documents, images, audio, video, etc. in one place.
8) Data security: Data security is one of the biggest advantages of cloud computing. Cloud offers many advanced features related to security and ensures that data is securely stored and handled.

Disadvantages of Cloud Computing:
A list of the disadvantage of cloud computing is given below -
1) Internet Connectivity: As you know, in cloud computing, every data (image, audio, video, etc.) is stored on the cloud, and we access these data through the cloud by using the internet connection. If you do not have good internet connectivity, you cannot access these data. However, we have no any other way to access data from the cloud.
2) Vendor lock-in: Vendor lock-in is the biggest disadvantage of cloud computing. Organizations may face problems when transferring their services from one vendor to another. As different vendors provide different platforms, that can cause difficulty moving from one cloud to another.
3) Limited Control: As we know, cloud infrastructure is completely owned, managed, and monitored by the service provider, so the cloud users have less control over the function and execution of services within a cloud infrastructure.
4) Security: Although cloud service providers implement the best security standards to store important information. But, before adopting cloud technology, you should be aware that you will be sending all your organization's sensitive information to a third party, i.e., a cloud computing service provider. While sending the data on the cloud, there may be a chance that your organization's information is hacked by Hackers.

Cloud Computing Architecture:
Cloud computing architecture is a combination of service-oriented architecture and event-driven architecture.
Cloud computing architecture is divided into the following two parts -
1)Front End: The front end is used by the client. It contains client-side interfaces and applications that are required to access the cloud computing platforms. The front end includes web servers (including Chrome, Firefox, internet explorer, etc.), thin & fat clients, tablets, and mobile devices.
2)Back End: The back end is used by the service provider. It manages all the resources that are required to provide cloud computing services. It includes a huge amount of data storage, security mechanism, virtual machines, deploying models, servers, traffic control mechanisms, etc.
Components of Cloud Computing Architecture
There are the following components of cloud computing architecture -
1. Client Infrastructure: Client Infrastructure is a Front end component. It provides GUI (Graphical User Interface)  to interact with the cloud.
2. Application: The application may be any software or platform that a client wants to access.
3. Service: A Cloud Services manages that which type of service you access according to the client’s requirement.
Cloud computing offers the following three type of services:
i. Software as a Service (SaaS) – It is also known as cloud application services. Mostly, SaaS applications run directly through the web browser means we do not require to download and install these applications. Some important example of SaaS is given below –
Example: Google Apps, Salesforce Dropbox, Slack, Hubspot, Cisco WebEx.
ii. Platform as a Service (PaaS) – It is also known as cloud platform services. It is quite similar to SaaS, but the difference is that PaaS provides a platform for software creation, but using SaaS, we can access software over the internet without the need of any platform.
Example: Windows Azure, Force.com, Magento Commerce Cloud, OpenShift.
iii. Infrastructure as a Service (IaaS) – It is also known as cloud infrastructure services. It is responsible for managing applications data, middleware, and runtime environments.
Example: Amazon Web Services (AWS) EC2, Google Compute Engine (GCE), Cisco Metapod.
4. Runtime Cloud: Runtime Cloud provides the execution and runtime environment to the virtual machines.
5. Storage: Storage is one of the most important components of cloud computing. It provides a huge amount of storage capacity in the cloud to store and manage data.
6. Infrastructure: It provides services on the host level, application level, and network level. Cloud infrastructure includes hardware and software components such as servers, storage, network devices, virtualization software, and other storage resources that are needed to support the cloud computing model.
7. Management: Management is used to manage components such as application, service, runtime cloud, storage, infrastructure, and other security issues in the backend and establish coordination between them.
8. Security: Security is an in-built back end component of cloud computing. It implements a security mechanism in the back end.
9. Internet: The Internet is medium through which front end and back end can interact and communicate with each other.

Cloud Computing Technologies:
A list of cloud computing technologies are given below -
1) Virtualization: Virtualization is the process of creating a virtual environment to run multiple applications and operating systems on the same server. The virtual environment can be anything, such as a single instance or a combination of many operating systems, storage devices, network application servers, and other environments.
The concept of Virtualization in cloud computing increases the use of virtual machines. A virtual machine is a software computer or software program that not only works as a physical computer but can also function as a physical machine and perform tasks such as running applications or programs as per the user's demand.
Types of Virtualization:
A list of types of Virtualization is given below -
-Hardware virtualization
-Server virtualization
-Storage virtualization
-Operating system virtualization
-Data Virtualization
2) Service-Oriented Architecture (SOA): Service-Oriented Architecture (SOA) allows organizations to access on-demand cloud-based computing solutions according to the change of business needs. It can work without or with cloud computing. The advantages of using SOA is that it is easy to maintain, platform independent, and highly scalable.
Service Provider and Service consumer are the two major roles within SOA.
Applications of Service-Oriented Architecture:
There are the following applications of Service-Oriented Architecture -
-It is used in the healthcare industry.
-It is used to create many mobile applications and games.
-In the air force, SOA infrastructure is used to deploy situational awareness systems.
3) Grid Computing: Grid computing is also known as distributed computing. It is a processor architecture that combines various different computing resources from multiple locations to achieve a common goal. In grid computing, the grid is connected by parallel nodes to form a computer cluster. These computer clusters are in different sizes and can run on any operating system.
Grid computing contains the following three types of machines -
-Control Node: It is a group of server which administrates the whole network.
-Provider: It is a computer which contributes its resources in the network resource pool.
-User: It is a computer which uses the resources on the network.
4) Utility Computing: Utility computing is the most trending IT service model. It provides on-demand computing resources (computation, storage, and programming services via API) and infrastructure based on the pay per use method. It minimizes the associated costs and maximizes the efficient use of resources. The advantage of utility computing is that it reduced the IT cost, provides greater flexibility, and easier to manage.
Large organizations such as Google and Amazon established their own utility services for computing storage and application.

Working of cloud computing:
Assume that you are an executive at a very big corporation. Your particular responsibilities include to make sure that all of your employees have the right hardware and software they need to do their jobs. To buy computers for everyone is not enough. You also have to purchase software as well as software licenses and then provide these softwares to your employees as they require. Whenever you hire a new employee, you need to buy more software or make sure your current software license allows another user. It is so stressful that you have to spend lots of money.
But, there may be an alternative for executives like you. So, instead of installing a suite of software for each computer, you just need to load one application. That application will allow the employees to log-in into a Web-based service which hosts all the programs for the user that is required for his/her job. Remote servers owned by another company and that will run everything from e-mail to word processing to complex data analysis programs. It is called cloud computing, and it could change the entire computer industry.

Platforms are used for large scale cloud computing:
The following platforms are used for large scale cloud computing:
-Apache Hadoop.
-MapReduce.

There are 3 layers in the hierarchy of cloud computing:
Infrastructure as a service (IaaS):It provides cloud infrastructure in terms of hardware as like memory, processor, speed etc.
Platform as a service (PaaS):It provides cloud application platform for the developer.
Software as a service (SaaS)::It provides the cloud applications to users directly without installing anything on the system. These applications remains on cloud.

Xaas in Cloud Computing:
"Anything as a service" (XaaS) describes a general category of cloud computing and remote access services. It recognizes the vast number of products, tools, and technologies now delivered to users as a service over the Internet.
Benefits of XaaS:
XaaS has many benefits: improving spending models, speeding up new apps and business processes, and shifting IT resources to high-value projects.
1) Expenditure model improvements. With XaaS, businesses can cut costs by purchasing services from providers on a subscription basis. Before XaaS and cloud services, businesses had to buy separate products-software, hardware, servers, security, infrastructure-install them on-site, and then link everything together to form a network. With XaaS, businesses buy what they need and pay on the go. The previous capital expenditure now becomes an operating expense.
2) Speed up new apps and business processes. This model allows businesses to adopt new apps or solutions to changing market conditions. Using multi-tenant approaches, cloud services can provide much-needed flexibility. Resource pooling and rapid elasticity support mean that business leaders can add or subtract services. When users need innovative resources, a company can use new technologies, automatically scaling up the infrastructure.
3) Transferring IT resources to high-value projects. Increasingly, IT organizations are turning to a XaaS delivery model to streamline operations and free up resources for innovation. They are also harnessing the benefits of XaaS to transform digitally and become more agile. XaaS gives more users access to cutting-edge technology, democratizing innovation. In a recent survey by Deloitte, 71% of companies report that XaaS now constitutes more than half of their company's enterprise IT.

Obstacles to cloud computing, including: 
- Security and privacy: Users may not know where their data is located, and they may not trust cloud service providers.
- Data security: Users are responsible for their data, but not all cloud providers can guarantee data security.
- Interoperability: It can be challenging to move applications between multiple cloud ecosystems.
- Compliance: It can be difficult to find a cloud provider that complies with public data policies.
- Lack of expertise: Organizations may not have the knowledge or tools to use cloud technology.
- Cloud management: Cloud architecture can be complex and challenging to maintain.

Traditional Computing : 
Traditional Computing, as name suggests, is a possess of using physical data centers for storing digital assets and running complete networking system for daily operations. In this, access to data, or software, or storage by users is limited to device or official network they are connected with. In this computing, user can have access to data only on system in which data is stored.

Advantages of Traditional Computing :
Control: With traditional computing, an organization has full control over the hardware and software it uses, allowing for customization and optimization of the computing environment.
Security: Traditional computing offers a high level of data security, as sensitive data can be stored on-premises and protected by firewalls, encryption, and other security measures.
Reliability: Traditional computing is not dependent on internet connectivity, making it less vulnerable to service disruptions or data loss.
Compatibility: Traditional computing environments can be tailored to meet the specific needs of an organization, ensuring compatibility with existing software and systems.
Data Ownership: With traditional computing, an organization owns and controls all of its data, reducing concerns about data privacy and regulatory compliance.

Disadvantages of Traditional Computing :
Cost: Traditional computing can be more expensive than cloud computing, as it requires significant capital expenditures for hardware and software, as well as ongoing maintenance and support expenses.
Scalability: Traditional computing can be difficult to scale up or down to meet changing needs, as it requires additional hardware or software to be added to the environment.
Accessibility: Traditional computing may not allow for remote access to applications and data, limiting the ability of users to work from anywhere.
Maintenance: Traditional computing environments require ongoing maintenance and upgrades to ensure security and performance, which can be time-consuming and expensive.
Limited Storage Capacity: Traditional computing environments may have limited storage capacity, requiring organizations to periodically purchase additional hardware to accommodate growing data volumes.

Cloud Computing vs Traditional Computing:
1)CC:It refers to delivery of different services such as data and programs through internet on different servers. TC:It refers to delivery of different services on local server. 
2)CC:It takes place on third-party servers that is hosted by third-party hosting companies. TC:It takes place on physical hard drives and website servers.  
3)CC:It is ability to access data anywhere at any time by user. TC:User can access data only on system in which data is stored.  
4)CC:It is more cost effective as compared to tradition computing as operation and maintenance of server is shared among several parties that in turn reduce cost of public services. TC:It is less cost effective as compared to cloud computing because one has to buy expensive equipment’s to operate and maintain server.  
5)CC:It is more user-friendly as compared to traditional computing because user can have access to data anytime anywhere using internet. TC:It is less user-friendly as compared to cloud computing because data cannot be accessed anywhere and if user has to access data in another system, then he need to save it in external storage medium.  
6)CC:It requires fast, reliable and stable internet connection to access information anywhere at any time. TC:It does not require any internet connection to access data or information.  
7)CC:It provides more storage space and servers as well as more computing power so that applications and software run must faster and effectively. TC:It provides less storage as compared to cloud computing.  
8)CC:It also provides scalability and elasticity i.e., one can increase or decrease storage capacity, server resources, etc., according to business needs. TC:It does not provide any scalability and elasticity.  
9)CC:Cloud service is served by provider’s support team. TC:It requires own team to maintain and monitor system that will need a lot of time and efforts.  
10)CC:Software is offered as an on-demand service (SaaS) that can be accessed through subscription service. TC:Software in purchased individually for every user and requires to be updated periodically.

Distributed Computing:
Distributed computing refers to a system where processing and data storage is distributed across multiple devices or systems, rather than being handled by a single central device. In a distributed system, each device or system has its own processing capabilities and may also store and manage its own data. These devices or systems work together to perform tasks and share resources, with no single device serving as the central hub.
One example of a distributed computing system is a cloud computing system, where resources such as computing power, storage, and networking are delivered over the Internet and accessed on demand. In this type of system, users can access and use shared resources through a web browser or other client software.
Components of Distributed Computing:
There are several key components of a Distributed Computing System-
-Devices or Systems: The devices or systems in a distributed system have their own processing capabilities and may also store and manage their own data.
-Network: The network connects the devices or systems in the distributed system, allowing them to communicate and exchange data.
-Resource Management: Distributed systems often have some type of resource management system in place to allocate and manage shared resources such as computing power, storage, and networking.
The architecture of a Distributed Computing System is typically a Peer-to-Peer Architecture, where devices or systems can act as both clients and servers and communicate directly with each other.
Characteristics:
There are several characteristics that define a Distributed Computing System-
-Multiple Devices or Systems: Processing and data storage is distributed across multiple devices or systems.
-Peer-to-Peer Architecture: Devices or systems in a distributed system can act as both clients and servers, as they can both request and provide services to other devices or systems in the network.
-Shared Resources: Resources such as computing power, storage, and networking are shared among the devices or systems in the network.
-Horizontal Scaling: Scaling a distributed computing system typically involves adding more devices or systems to the network to increase processing and storage capacity. This can be done through hardware upgrades or by adding additional devices or systems to the network.
Advantages and Disadvantages Distributed Computing:
Advantages of the Distributed Computing System are-
-Scalability: Distributed systems are generally more scalable than centralized systems, as they can easily add new devices or systems to the network to increase processing and storage capacity.
-Reliability: Distributed systems are often more reliable than centralized systems, as they can continue to operate even if one device or system fails.
-Flexibility: Distributed systems are generally more flexible than centralized systems, as they can be configured and reconfigured more easily to meet changing computing needs.
There are a few limitations to Distributed Computing System-
-Complexity: Distributed systems can be more complex than centralized systems, as they involve multiple devices or systems that need to be coordinated and managed.
-Security: It can be more challenging to secure a distributed system, as security measures must be implemented on each device or system to ensure the security of the entire system.
-Performance: Distributed systems may not offer the same level of performance as centralized systems, as processing and data storage is distributed across multiple devices or systems.

DISTRIBUTED COMPUTING vs CLOUD COMPUTING:
1)Distributed computing is solving a difficulty using distributed autonomous machines and communicating with each other over a network.	In cloud computing, IT resources and services like servers, storage, databases, networks, analytics, software, etc. are provided over the Internet.
2)In basic distributed computing, a computing method may be used to communicate and solve one single problem by numerous machines. A basic cloud computing technology may provide host services to your users/customers over the Internet.	
3)It is divided into three main types: distributed computing systems, distributed systems, and distributed power systems.	CC is divided into 4 kinds – Public, Private, Hybrid, and Community Cloud. It has a varied ranking.
4)Distributed computers have numerous advantages, such as flexibility, dependability, increased performance, etc.	Cloud computing has numerous advantages, including cost efficiency, flexibility, and reliability, scaling economies, world market access, etc.
5)Distributed computing helps more quickly than utilizing a single computer when it takes much time to complete the computing activities.	Cloud computing delivers services, including hardware, software, and Internet networking resources.
6)The objective of distributed computing is to distribute and complete a single job amongst several computers fast through cooperation between machines.	Internet Pay-per-Use Computing Services are available on request from the cloud computing service providers.
7)Some distributed computing features spread a single job amongst the machines to simultaneously advance the work, remote procedure calls, and the remote method invocation for distributed calculations.	Some of the features of cloud computing include pooled computer resources, on-demand service, pay-per-usage, service providers, etc.
8)Some cloud computing disadvantages include the possibility of a node failure, and sluggish connectivity might lead to problems.	Some cloud computing disadvantages may include less control, particularly public clouds, limitations on existing services, and cloud security.

Advantages of distributed computing over traditional computing
1)Scalability:
Traditional Computing: Upgrading hardware for scaling.
Distributed Computing: Horizontal scaling for cost-effective expansion.
2)Fault Tolerance:
Traditional Computing: Single point failure risks.
Distributed Computing: Redundancy minimizes downtime and data loss.
3)Performance Optimization:
Traditional Computing: Limited by single machine capacity.
Distributed Computing: Parallel processing enhances speed.
4)Resource Utilization:
Traditional Computing: Centralized resources may be underutilized.
Distributed Computing: Dynamic allocation improves efficiency.
5)Geographical Distribution:
Traditional Computing: Limited to a single location.
Distributed Computing: Enables global resource distribution.

Grid Computing: 
Grid computing is a distributed structure of huge number of machines situated in multiple locations, connected to solve a complex problem. In the grid computing paradigm, personal computers or servers run independent tasks and are loosely connected by the Internet or low- speed networks.
It is a Distributed computing architecture. In this computing, resources are used in collaborative pattern, and also in grid computing, the users do not pay for use.
Applications of Grid Computing:
One of the prime grid computing strategies is to utilize middleware to divide and allocate chunks of a program among several machines, at times, up to several thousands. Being a distributed computation technique, grid computing involves the aggregation of the large- scale clusters. However, the size of a grid may vary from small to large.
1)Partitioning an application where it is required to further break a problem into discrete chunks
2)Identifying and scheduling tasks
3)Distributing problem data as and when it is needed
4)Distributing application codes to specific machine nodes
5)Assisting in result management and decision making processes
6)Facilitating self-optimization, self-configuration, self-management and self-recovery

Cloud Computing vs Grid Computing:
1. Cloud computing is a Client-server computing architecture.	While Grid computing is a Distributed computing architecture.
2. Cloud computing is a centralized executive. While grid computing is a decentralized executive.
3. In cloud computing, resources are used in centralized pattern. While in grid computing, resources are used in collaborative pattern.
4. It is more flexible than grid computing.	While Grid computing is less flexible than cloud computing.
5. In cloud computing, the users pay for the use. While in grid computing, the users do not pay for use.
6. Cloud computing is a high accessible service. While grid computing is a low accessible service.
7. It is highly scalable as compared to grid computing. While grid computing is low scalable in comparison to cloud computing.
8. It can be accessed through standard web protocols.	While it is accessible through grid middleware.
9. Cloud computing is based on service-oriented. Grid computing is based on application-oriented.
10. Cloud computing uses service like IAAS, PAAS, SAAS. Grid computing uses service like distributed computing, distributed pervasive, distributed information.


Service models in Cloud computing:

1)SaaS: 
SaaS is also known as "On-Demand Software". It is a software distribution model in which services are hosted by a cloud service provider. These services are available to end-users over the internet so, the end-users do not need to install any software on their devices to access these services. 
Examples: Examples of SaaS include Google Apps, Microsoft Office 365, Onlive, GT Nexus, Marketo, Slack, Samepage, Box, and Zoho Forms.
Advantages: 
-Cost-Effective: Monthly or annual subscription fees make SaaS more affordable than traditional licensed applications.
-One-to-Many Model: Allows multiple users to share a single instance of the application.
-Less Hardware Required: Remote hosting eliminates the need for additional hardware investments.
-Low Maintenance: No installation or daily maintenance; automatic updates and easy monitoring.
-Uniformity: All users access the same version through a web browser, reducing IT support costs.
-Multidevice Support: Accessible from desktops, laptops, tablets, phones, and thin clients.
-API Integration: Easily integrates with other software or services through standard APIs.
-No Client-Side Installation: Accessed directly through the internet, eliminating the need for software installation.
Disadvantages:
-Security Concerns: Data stored in the cloud may pose security issues, although cloud security is on par with in-house deployments.
-Latency Issues: Variable distance to cloud storage may result in greater latency for some applications.
-Dependency on Internet: Most SaaS applications are unusable without an internet connection.
-Vendor Switching Difficulty: Transferring large data files and converting/importing into another SaaS can be slow and challenging.
Architecture: In the SaaS model, Cloud providers install and operate application software in the Cloud and Cloud users access the software from Cloud clients. Cloud users do not manage the Cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the Cloud user's own machine, which simplifies maintenance and support. Cloud applications are different from other applications in their scalability-which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demands. Load balancers distribute the work over the set of virtual machines. This process is transparent to the Cloud user, who sees only a single access point. To accommodate a large number of Cloud users, Cloud applications can be multi-tenant, that is, any machine serves more than one Cloud user organisation.
SaaS applications run on distant but networked computers in the cloud. These services and distant computers are operated and owned by other people or organisations that connect to the users' gadget through the Internet.

2)PaaS:
PaaS, or Platform as a Service, is a cloud computing model where cloud providers deliver a computing platform, including the operating system, programming language execution environment, database, and web server, as a service. It facilitates application development and deployment without the need for organizations to invest in and manage underlying hardware and software infrastructure. PaaS providers handle scalability, allowing resources to automatically adjust to user needs.
Examples: Examples of PaaS include AWS Elastic Beanstalk, Cloud Foundry, Heroku, Force.com, EngineYard, Mendix, OpenShift, Google App Engine, Windows Azure Cloud Services and OrangeScape.
Advantages:
-Rapid Development: Enables faster application design and development, leading to quicker time-to-market.
-Efficiency in Deployment: Allows users to upload new web applications to the cloud within minutes, reducing deployment time.
-Cost Savings: Eliminates the need for organizations to invest in and maintain underlying hardware and software layers, reducing costs.
-Automatic Scalability: Resources scale automatically based on user needs, providing flexibility and efficiency.
-Decreased Complexity: Middleware is provided as a service, reducing the complexity of managing infrastructure components.
-Streamlined Maintenance: PaaS providers handle maintenance tasks, freeing users from routine updates and management responsibilities.
Disadvantages:
-Limited Control: Users have less control over the underlying infrastructure, as it is managed by the PaaS provider.
-Vendor Dependency: Organizations become dependent on PaaS providers for crucial services, and switching providers can be challenging.
-Security Concerns: Entrusting sensitive data to a third-party provider may raise security concerns, although providers implement security measures.
-Compatibility Issues: Applications developed on one PaaS platform may face compatibility challenges if migrated to a different provider's platform.
-Customization Constraints: Some PaaS platforms may limit customization options, impacting the ability to tailor solutions to specific needs.
-Internet Dependency: Continuous internet access is essential for using PaaS, and disruptions may hinder access to applications and services.
Architecture: In the PaaS model, cloud providers deliver a computing platform typically including operating system (OS), programming language execution environment, database and Web server. Application developers can develop and run their software solutions on a Cloud platform without the cost and complexity of buying and managing the underlying hardware and software layers. With some PaaS offers, the underlying computer and storage resources scale automatically to match user needs.

3)IaaS:
IaaS, or Infrastructure as a Service, is a cloud computing service model where cloud providers provision server, storage, and database services to customers. It enables real-time scaling of computing and storage resources based on usage. In IaaS, providers offer virtual machines (VMs), either physical or virtual, and additional resources like storage, firewalls, load balancers, IP addresses, and software bundles. Users deploy their applications by installing operating system images and application software on the cloud infrastructure, taking responsibility for patching and maintaining the software.
Examples: Examples of laas providers include Amazon EC2, Dyn DNS, HP Cloud, iland, Joyent & Terremark.
Advantages: 
-No Hardware Investment: Users do not need to invest in their own hardware, reducing upfront costs.
-Scalability: Infrastructure can be scaled on demand to support dynamic and varying workloads.
-On-Demand Services: Innovative and flexible services are available on demand, allowing for agility in resource utilization.
-Dynamic Scaling: Resources can be dynamically scaled based on changing requirements, providing flexibility.
-Automation: Administrative tasks are automated, streamlining operations and reducing manual intervention.
-Pay-as-You-Go Model: Payment is based on a utility computing basis, allowing users to pay for resources consumed without large upfront costs.
Disadvantages: 
-Responsibility for Maintenance: Users are responsible for patching and maintaining operating systems and application software.
-Security Concerns: Security risks exist as users entrust their data and applications to a third-party provider.
-Learning Curve: Users need to adapt to the cloud environment and manage their infrastructure effectively.
-Potential Overhead Costs: Depending on usage, costs can accumulate, and unexpected spikes in usage may lead to higher bills.
-Dependency on Internet Connectivity: Continuous internet access is crucial, and disruptions may impact access to services.
-Customization Challenges: Some IaaS providers may have limitations on customization, affecting specific application requirements.
Architecture: 
System Architecture of IaaS-
Layer 1: User Interface (Web Browser, Application Interface)-
Web Browser: Users interact with the IaaS system through a web browser, providing a user-friendly interface for accessing and managing resources.
Application Interface: Offers programmatic interfaces (APIs) for automation and integration, enabling users to interact with the IaaS platform programmatically.
Layer 2: Middleware (Management and Automation)-
Management Layer: Responsible for handling user requests, resource provisioning, and overall system management. It includes components for user authentication, authorization, and resource allocation.
Automation Layer: Implements automation scripts and tools for tasks such as scaling, load balancing, and resource optimization. This layer streamlines the management of IaaS resources.
Layer 3: Virtual Machines (VMs), Storage, Networking, and Computing-
Virtual Machines (VMs):
-Hypervisor: Manages the creation and operation of virtual machines, enabling multiple operating systems to run on a single physical host.
-VM Instances: Instances of virtual machines that provide computing power and emulate specific operating system environments.
Storage:
-Block Storage: Offers scalable, raw storage blocks for VMs, providing flexibility in storage allocation.
-Object Storage: Manages and stores data as objects, suitable for unstructured data and file storage.
Networking:
-Virtual Networks: Facilitates communication between VMs and external networks, ensuring connectivity.
-Load Balancers: Distribute network traffic evenly across multiple VMs, optimizing performance and ensuring high availability.
Computing:
-Physical Servers: The underlying hardware that hosts VMs and provides computational resources.
-Resource Allocation: Allocates computing resources (CPU, memory) to VMs based on user demand and application requirements.

4)HaaS:
Hardware-as-a-Service (HaaS) is a managed service provided by cloud service providers, where IT facilities, including hardware components such as servers and data centers, are leased to clients. This model allows clients to access and utilize computing resources without the need for direct involvement in hardware provisioning, maintenance, and operations. HaaS is akin to utilities providing electricity, water, or gas, where the focus is on consuming services rather than owning and managing the underlying hardware infrastructure. 
Examples: Cab and bike rental services, IT leasing services, and Office equipment such as printers, copiers, and fax machines.
Advantages: 
-Cost Efficiency: Eliminates the need for upfront investment in hardware, reducing capital expenditures.
-Scalability: Provides the flexibility to scale hardware resources up or down based on business needs.
-Focus on Application Development: Allows clients to concentrate on application development without the burden of managing hardware infrastructure.
-Accessibility: Applications developed using HaaS can be accessed from anywhere, promoting remote and flexible work arrangements.
-Virtualization Benefits: Utilizes virtualization technology to optimize resource allocation and enhance efficiency.
-Defined Service Level Agreements (SLAs): Establishes clear agreements between users and providers regarding hardware availability and related services.
-Pay-as-You-Go Model: Offers a pay-as-you-go pricing model, allowing users to pay for hardware resources based on actual usage.
Disadvantages:
-Dependency on Service Provider: Users rely on the service provider for hardware provisioning, maintenance, and replacement, leading to potential dependencies.
-Security Concerns: Entrusting hardware to a third-party provider may raise security and data privacy concerns.
-Limited Control: Users may have limited control over the underlying hardware infrastructure, impacting customization options.
-Potential for Downtime: Service interruptions or hardware failures at the provider's end can affect the availability of applications.
-Monthly Fees: While eliminating upfront costs, monthly fees may accumulate over time, and unexpected usage spikes can result in higher bills.
-Compatibility Issues: Transitioning between HaaS providers or integrating with existing on-premises infrastructure may pose compatibility challenges.
Architecture:
1.Client Interface: User portal for managing hardware resources.
2.Service Orchestration:
-Orchestration engine for resource allocation.
-Automation scripts for task optimization.
3.Resource Pool: Physical hardware and virtualization layer.
4.Monitoring and Management:
-Monitoring tools for performance tracking.
-Management console for administration.
5.Security Layer: Access controls and encryption measures.
6.Billing and Metering: Billing system and metering tools for usage transparency.
7.Integration APIs: APIs for external system connectivity.
8.Support and Maintenance:
-Helpdesk and support services.
-Defined maintenance procedures.
9.Backup and Disaster Recovery: Strategies for data backup and recovery.
10.Compliance and Governance: Measures for regulatory compliance and data governance.

Relationship Between SaaS, PaaS, and IaaS:
IaaS (Infrastructure as a Service): Provides foundational computing resources like virtual machines and storage.
PaaS (Platform as a Service): Builds on IaaS, offering a development platform with tools and frameworks.
SaaS (Software as a Service): Utilizes PaaS for application development and runs on the infrastructure provided by IaaS.
Relationship:
- IaaS and PaaS: PaaS relies on IaaS for foundational infrastructure.
- PaaS and SaaS: SaaS can be built on PaaS, leveraging development frameworks.
- IaaS and SaaS: IaaS indirectly supports SaaS by providing essential computing resources.

Types of Cloud/deployment models:
1) Public Cloud:
Definition: Public Cloud refers to a cloud infrastructure where services and resources are provided over the internet by third-party cloud service providers. It is open for use by the general public.
Example: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP).
Advantages:
-Cost-Effective: Users only pay for the resources they use.
-Scalability: Easily scalable to accommodate varying workloads.
-Accessibility: Services are accessible from anywhere with an internet connection.
Disadvantages:
-Security Concerns: Potential security and privacy risks due to the shared nature of the infrastructure.
-Limited Customization: Limited control and customization compared to private cloud solutions.
2) Private Cloud:
Definition: Private Cloud involves the exclusive use of cloud resources by a single organization. The infrastructure may be on-premises or hosted by a third-party provider.
Example: VMware Cloud Foundation, OpenStack for private clouds.
Advantages:
-Enhanced Security: Greater control over security measures and data privacy.
-Customization: Tailored to meet specific organizational needs.
-Compliance: Easier adherence to industry-specific compliance requirements.
Disadvantages:
-Cost: Typically involves higher upfront costs compared to public cloud solutions.
-Complexity: Requires skilled IT personnel for management and maintenance.
3) Hybrid Cloud:
Definition: Hybrid Cloud combines both public and private cloud environments, allowing data and applications to be shared between them. It provides greater flexibility and optimization of existing infrastructure.
Example: Microsoft Azure with on-premises data centers, AWS with a combination of public and private resources.
Advantages:
-Flexibility: Can leverage the benefits of both public and private clouds.
-Scalability: Easily scale resources based on fluctuating workloads.
-Risk Mitigation: Offers redundancy and risk diversification.
Disadvantages:
-Complexity: Managing a hybrid environment can be more complex.
-Cost: Cost implications associated with both public and private cloud usage.
4) Community Cloud:
Definition: Community Cloud is a shared cloud infrastructure that is used by multiple organizations with common interests or requirements, such as regulatory compliance.
Example: A community cloud used by multiple healthcare organizations sharing patient data.
Advantages:
-Collaboration: Facilitates collaboration among organizations with shared goals.
-Cost Sharing: Shared infrastructure leads to potential cost savings.
-Customization: Tailored to meet the specific needs of the community.
Disadvantages:
-Limited Scope: May not be suitable for organizations with diverse needs.
-Dependency: Relies on effective collaboration and agreement among community members.


Public Cloud vs	Private Cloud:
1) A public cloud is a computing environment in which computing infrastructure and resources are shared with the public over the Internet. A private cloud is a computing infrastructure and resource-sharing network that is connected via the Internet to a private network.
2) Public cloud functions on the principle of storage demand scalability.	Private cloud can only be shared among users of an organization.
3) Public cloud deployment control over infrastructure is limited. Private cloud environment provides complete control.
4) Public cloud is connected to the public Internet.	Private cloud only provides connectivity over the private network.
5) Public cloud is hosted at the Service Provider site.	Private cloud is hosted at the Service Provider site or enterprise.
6) Public cloud is an affordable solution that provides room for growth. Private cloud Provides	high performance, security, customization, and control options.
7) Public cloud is widely used to provide web-based email, online storage, application testing, and development environments. Private cloud	uses widely in the application to protect our most sensitive data and applications.
8) Public cloud is managed by Cloud Service Provider’s technical team. Private cloud is managed by In-house technical administrators.
9) A public cloud is a multi-tenant in which your service provider manages the network.	A private cloud is like a single tenant in which the in-house team handles the network.
10) Public cloud is an affordable option offering a pay-as-you-go service fee. Private cloud requires large upfront costs for implementing the hardware, software, staff, and other resources.
11) Users have to pay a monthly bill for public cloud services.	In the Public cloud, money is charged based on usage GB and bandwidth transfer fees.
12) In Public cloud, reliability is moderate here. Private infrastructure offers a high level of reliability.
13) In-house software not needed in Public cloud computing model. Very high in-house software requirement in Private cloud computing model.
14) In Public cloud, Only offers essential security compliance. In Private cloud, Enhanced security to meet data protection legislation.
15) In the public cloud, the performance is low to medium.	The performance is high in a private cloud.
16) It covers the shared servers.	It covers the devoted servers.
17) A public cloud is comparable to renting a truck from a large company that is available to everyone.	The concept of a private cloud can be compared to owning and managing a fleet of trucks.
18) Public cloud Providers- Amazon web service (AWS) and Google AppEngine, etc. Private cloud Providers- Microsoft KVM, HP, Red Hat & VMWare, etc.


Middleware:
Middleware acts as a bridge between diverse technologies, tools, and databases so that you can integrate them seamlessly into a single system. The single system then provides a unified service to its users.
Middleware is software that lies between an operating system and the applications running on it. Essentially functioning as hidden translation layer, middleware enables communication and data management for distributed applications.
In cloud computing, middleware refers to the software layer that facilitates communication and interaction between different applications, services, and components within a cloud environment. It acts as an intermediary layer, connecting and enabling seamless communication between various software applications and the underlying infrastructure. Middleware plays a crucial role in integrating, managing, and coordinating distributed systems and services in a cloud-based architecture.
Middleware is software and cloud services that provide common services and capabilities to applications and help developers and operators build and deploy applications more efficiently. Middleware acts like the connective tissue between applications, data, and users.
Middleware has been part of software engineering terminology since the late 1960s, and as a category can apply to a wide range of modern software components. Middleware can include application runtimes, enterprise application integration and various kinds of cloud services. Data management, application services, messaging, authentication, and application programming interface (API) management are all commonly handled by middleware.
Examples include message-oriented middleware, enterprise service bus, and application servers. Middleware is crucial for building and managing robust cloud-based systems.


Google Cloud Platform / Infrastructure:
Today, Google is one of the prime cloud service platforms in close competition with Microsoft Azure and AWS. In the year 2008 Google initiated Google App engine i.e. its first cloud tool; and henceforth continued adding more services and tools that collectively shaped into Google Cloud Platform.
Google’s cloud computing platform provides a competent infrastructure product-and-service suite offering cloud computing solutions, cloud storage, application hosting, database services, data storage, prediction APIs, translations APIs and much more.
This also adds to the benefits of the Google cloud infrastructure:
1. Compute: App Engine, Cloud functions, Container registry, Compute engine and container engine
2. Storage: Cloud Bigtable, Cloud SQL, Cloud Storage, Cloud Datastore
3. Networking: Cloud Virtual networking, Cloud CDN(Content Delivery network), Cloud Load Balancing, DNS and Interconnect
4. Big Data: BigQuery, Datalab, Dataproc, Dataflow, Pub/Sub and Genomics
5. Machine learning: ML Platform, Speech API, Vision API and the Translate API
6. Management tools: Monitoring, Stackdriver Overview, error Reporting, Logging, Trace, Debugger, Cloud console, Cloud mobile app, Deployment Manager, Cloud shell, Billing API.
7. Developer tools: Cloud SDK, Cloud Source Repositories, Cloud Tests Lab, Deployment Manager, Endpoints, IntelliJ, Tools for android studio, Google plug-in for Eclipse,.
8. Identity and Security: Cloud IAM, Security Scanner, Resource Manager, Platform Security overview.

Google File System (GFS or Google FS):
Google File System is a type of distributed file system created by Google to meet the company’s fast growing data processing demands. Also known as Google FS or GFS, it is Google’s proprietary product and provides the benefits of availability, scalability, performance and reliability. It is used to efficiently manage massively big amounts of data. It also supports the developers to research and access the desired resources in creating the apps.

GFS – Features:
1) High availability.
2) Fault tolerance.
3) Automatic and efficient data recovery High aggregate throughput.
4) High aggregate throughput.
5) Critical data replication.
6) Namespace management and locking.
7) Minimal client-master interaction because of large chunk server size.

Composition of GFS:
GFS is composed of numerous storage systems built from low-priced commodity hardware components. Files on GFS are very heavy, usually in multi-gigabyte (GB) range. Accessing such large-sized files for processing require a huge network bandwidth (i.e. the capability of a system to move data between different locations). GFS easily sorts out this problem. It breaks the huge files into small chunks of 64 megabytes (MB) each. Each chunk is given a distinct 64-bit identification number which is known as a chunk handle.
The task of balancing the workload among the systems is simplified through easy porting of file chunks from one resource to another.

Working of GFS:
• Initially a client sends a request to the Master to search the location of the chunk server which is acting as the primary duplicate.
• The Master identifies the primary duplicate and sends the location of all the chunk server duplicates to the client.
• The client stacks the data and allocates the write data task to the chunk server duplicates, beginning with the nearest.
• After the duplicates obtain the data, the client communicates with the primary duplicate to begin the write function.
• The primary duplicate writes the data to the appropriate chunk and signals the secondary duplicates to do the same.
• The secondary duplicates execute the write data task and address back to the primary duplicate.
• Finally, the primary duplicate sends a confirmation message regarding the completion of the write data task to the client.

Advantages of GFS:
-High Availability: GFS ensures data remains accessible despite server failures.
-Fault Tolerance: It tolerates faults with automatic and efficient data recovery.
-Scalability: Efficiently manages massive amounts of data, meeting growing demands.
-High Aggregate Throughput: Optimized for fast access to large files and datasets.
-Critical Data Replication: Supports duplication of critical data for fault tolerance.
-Performance: GFS delivers high performance, especially in handling large-scale data processing tasks.

Disadvantages of GFS:
-Proprietary Nature: GFS is proprietary, limiting transparency and customization.
-Complexity: Setting up and managing GFS may be challenging due to its complexity.
-Large Chunk Server Size Interaction: Challenges in efficiently managing large chunks, especially with varying file sizes.
-Limited Interoperability: Optimized for Google's needs, may not seamlessly integrate with other systems.
-Bandwidth Requirements: Accessing large files may demand significant network bandwidth.

Google Search Engine:
With the acquisition of major market share, Google search engine places itself as world’s most widely accepted and powerful search engine. Google makes use of a special trade marked algorithm known as PageRank to find the search results. The algorithm’s generics are transparent to the world but the specifics are undisclosed and remain a secret. This maintains the company’s competitive edge.
Google makes use of robotic programs in the search result generation process. These are known as spiders or crawlers. A spider crawls up the web and builds a list based upon the content to be searched. This process is known as Spidering or Web crawling. The spiders record the words entered in the search engine and then create a search keyword to find the related web pages. This makes Google possess a huge index of search keywords and their location.
Google’s algorithm assigns each web page a relevancy score or a rank. The way Google ranks the search results makes it distinct from other search engines. The rank in turn determines the sequence in which Google displays the results on its search engine results page (SERP).

Web Page ranking Factors:
1) The location and frequency of keywords on a webpage.
2) The duration a web page exists.
3) The number of other linked web pages.
4) Backlinks: The number and quality of external websites linking to a particular webpage.
5) Content Quality: Relevance, uniqueness, and overall quality of the content on a webpage.
6) User Engagement Metrics: Factors such as click-through rate, bounce rate, and dwell time contribute to page ranking.
7) Mobile Friendliness: How well a webpage performs and is designed for mobile devices.
8) Page Loading Speed: The time it takes for a webpage to load, as it affects user experience and SEO.
9) Social Signals: The presence and impact of social media shares, likes, and comments related to a webpage.
10) Domain Authority: The overall authority and credibility of the domain hosting the webpage, as determined by various factors.

Map Reduce and Map Reduce framework:
This is the digital era and data’s growth is massively speedy. This brings in the requirement for a highly efficient mechanism to manipulate such huge amounts of data. However, this entire task can be extremely tedious demanding numerous amount of time spent over solving recurring problems.
Problems such as processing in parallel, data distribution and majorly handling errors during processing need meticulous execution. Therefore, MapReduce framework was introduced by Google to make the developers get rid of such recurring and time-consuming tasks.
As the name suggests, MapReduce is a programming paradigm based on the principle of functional programming. Excessive amount of computations can be executed in parallel over multiple systems (collectively known as a cluster) to process massive amount of data sets.
The user has to specify a map function and a reduce function. Map function is used to process a key/value pair for generating a set of intermediate key/value pairs. Reduce function is used to merge all the intermediate values that are associated with the same intermediate key.

Execution of MapReduce / Phases of MapReduce:
Google created a layer of abstraction to split the flowing data into 2 major phases:
1) The Map Phase–The Map phase may include many map functions. A map function
refers to a high-order function that applies a given function to each element of a list.
2) The Reduce Phase–The Reduce phase may include many reduce functions. A reduce function refers to a high-order function that processes the result from the map phase by combining it into a consolidated form. This consolidated result is actually the final output.

Components of MapReduce Framework:
The MapReduce framework consists of several basic components that work together to process and analyze large-scale data in a distributed computing environment:
1. Mapper:
Functionality: Processes input data and generates a set of intermediate key/value pairs.
Responsibility: Distributes the workload across multiple nodes in the cluster.
2. Reducer:
Functionality: Aggregates and consolidates the intermediate key/value pairs produced by the mappers to generate the final output.
Responsibility: Performs the reduction and combines results.
3. InputFormat:
Functionality: Specifies how input data is divided and processed by the mappers.
Responsibility: Defines the structure and organization of the input data.
4. OutputFormat:
Functionality: Specifies the format of the final output generated by the reducers.
Responsibility: Defines how the results are formatted and presented.
5. Shuffle and Sort:
Functionality: The process of redistributing and sorting the intermediate key/value pairs before they are sent to the reducers.
Responsibility: Ensures that all values associated with a particular key are sent to the same reducer.
6. Partitioner:
Functionality: Determines how the intermediate key/value pairs are distributed to different reducers.
Responsibility: Ensures an even distribution of data and workload among reducers.
7. JobTracker:
Functionality: Manages and coordinates the execution of MapReduce jobs.
Responsibility: Schedules tasks, monitors progress, and handles job failures.
8. TaskTracker:
Functionality: Executes individual tasks (map or reduce) on a specific node in the cluster.
Responsibility: Reports task progress and status to the JobTracker.
9. Combiner:
Functionality: An optional function that performs a local reduction on the output of each mapper before it is sent to the reducers.
Responsibility: Reduces the amount of data transferred over the network during the shuffle and sort phase.

A real world analogy of MapReduce: 
You have 50 bags full of assorted chocolates of different brands. You want to count the total number of chocolates of each brand. If you start counting serially and follow the conventional method, it is going to be extremely complex and frustrating.
The MapReduce method suggests the following:
• Communicate with 50 friends
• Allocate 1 bag to each friend
• Tell each friend to count the total number of chocolates of different brand names
• Receive the resultant information from each friend
• Record and aggregate the total count received
Your friends acted as map function and you acted as reduce function.
In case, any of the friends leave, you can re-assign the task to a new friend. This is one major benefits of the MapReduce framework, known as fault tolerance. A task can be easily transferred from one node to another.

Amazon Web Services (AWS):
Amazon Web Services (AWS) is a popular example of IaaS model. It was designed as an extended service of Amazon.com a few decades ago. At present, AWS is a rapidly growing organization with an extraordinary turnover of $10 billion. This turnover is above 30% of the market share and more than its competitors – Google, IBM and Microsoft.
According to Amazon, AWS is a secure cloud services platform that helps businesses scale and grow.
AWS cloud offers several benefits of infrastructure-as-a-service:
• Database storage
• Computing power
• Content delivery
• Networking and databases delivered as a utility: this is available on-demand, in seconds, with pay-as-you-go pricing
Following are the chief Amazon web services components:
• Amazon EC2
• Amazon RDS
• AWS Direct Connect
• Amazon EBS
• Amazon S3
• Elastic Load Balancing
• Amazon Route 53
• Amazon VPC
• Elastic IP
A huge number of clients utilize AWS cloud products and solutions to develop refined applications with enhanced reliability, flexibility and scalability. AWS provides a complete cloud infrastructure ready to be used for any workload. 

Different types of APIs-
API refers to Application Programming Interface. It is a set of protocols or standards that are used in developing a software application. There exist different types of APIs that are used to build web applications, operating systems and even websites.
(i) Rest API: REST refers to Representational State Transfer. It is a type of Web service API. It utilizes standards like URL, JSON, XML and HTTP for transmitting messages. REST is an architectural technique and does not follow any specific official standard. It promotes the ease of use, visibility, reliability, scalability, performance and modifiability. REST is utilized by the World Wide Web.
(ii) Soap API: SOAP refers to Simple Object Access Protocol. It is also a type of web service API that utilizes standards like SMTP, XML, HTTP, TCP, JMS and UDP for message transmission. SOAP is used to exchange structured information during the implementation of web services. The characteristics of SOAP are neutrality, independence and extensibility. Unlike Rest API, SOAP API follows official standards.
(iii) Query API: Query API refers to querying a data source for information about its entities. For example, if the data source contains university entities, you can query for university offering various courses nearby or may be for a particular location. Each query response returns corresponding results including various values.

User Authentication:
User authentication is a mechanism that allows a machine to validate the identity of whosoever connects to the network. It is very essential that a network remains safe and secure to majorly avoid the loss of confidential data. Therefore, it is essential to perform user authentication process.
To enable the access of only authorized users, there are various authentication methods:
• Biometrics
• Computer recognition software
• One Time Password (OTP) token
• SMS or e-mail one-time password (OTP)
• Peripheral device recognition
• Out of band
• Scratch-off card
One real time example of user authentication is the case of accessing your Google mail account. It is mandatory for you to enter your correct user name and password. When the username and password provided by you matches with the registered credentials in the Google database, the system accepts you as an authentic user.

Connecting to the Cloud:
Connecting to the cloud is a general term used to denote the instance when you are connected to a network that has multiple of shared resources to offer. A real time example is connecting to the Wi-Fi (Wireless Fidelity) network or to the network provided by an internet service provider (ISP).
Cloud is the group of shared resources available to the user. They may be software such as mobile apps or hardware devices such as a digital camera or an I-pad.
In case of a Wi-Fi network, search for the available networks. Once you find an open and secure network, click to connect to the network. Provide the required password and a successful connection can be established over the cloud.
In case of a network provided by an Internet service provider, search for the specific icon or switch to be turned on. Once it is on, you will be able to connect to your registered network and access the Internet.

OpenSSH Keys:
Secure Shell (SSH) protocol suite offers OpenSSH as a free tool to remotely monitor and transfer files between computers in a secured manner. RCP or Telnet are conventional techniques that are used for the same purpose but they are insecure because they transfer the confidential information such as user’s password in clear text format. OpenSSH is used to efficiently enforce secure, encrypted file transfer and remote monitoring functions thereby replacing the conventional tools.

Set Up OpenSSH Keys:
OpenSSH keys are set up by using public-private key-pairs. This method is safe and secure as there is no need of typing the password each time. The keys-pairs can be generated by using either of the algorithms, i.e. DSA or RSA.
The following command is used to generate public/private dsa key pair:
% ssh-keygen -t dsa
The following command is used to generate public/private dsa key pair:
% ssh-keygen -t rsa

Benefits of using OpenSSH:
- Encrypted Communication: OpenSSH ensures secure, encrypted data transfer and remote monitoring.
- Passwordless Authentication: OpenSSH keys enable secure logins without typing passwords each time.
- Key-Pair Security: Public-private key-pairs add an extra layer of authentication and protection.
- Versatile Protocols: OpenSSH supports secure protocols for diverse operating systems and network environments.
- Replace Insecure Protocols: OpenSSH replaces insecure methods like RCP and Telnet, offering a more robust and secure alternative.

Tunneling / Port Forwarding:
Tunneling or port forwarding is a process of data transmission that is carried out within a private network, majorly business network using a public network. In this process, the public network routing nodes do not come to know that the ongoing data transmission is an action of a private network.
-Point-to-Point Tunneling Protocol (PPTP) is a widely used tunneling mechanism which is built by Microsoft. Authorized users can log into a virtual private network (VPN) by using the support of PPTP.
-Another popular tunneling protocol is generic routing encapsulation (GRE) which is built by Cisco Systems.

The Working of Tunneling:
The task of Tunneling is carried out by encapsulating the protocol data of the public network transmission and the private network data. This makes private network protocol data appears to be data in general for the public network.
The main purpose of port forwarding is to retain the confidentiality of the private network data thereby making the information secure. However, it does not serve as a substitute for high level security measures related to encryption or decryption.

Benefits of Tunneling:
- Secure Data Transmission: Tunneling ensures secure data transmission within private networks over a public network.
- VPN Access: Allows authorized users to log into a virtual private network (VPN) using protocols like PPTP.
- Network Privacy: Maintains the confidentiality of private network data by encapsulating it within the public network.
- Protocol Agnosticism: Supports various tunneling protocols like PPTP and GRE for flexible and efficient data transmission.
- Enhanced Confidentiality: Port forwarding helps in retaining the confidentiality of private network data, adding an extra layer of security.


Three Ways to control access to Google cloud storage and objects:
Here are some ways to control access to Google Cloud Storage and objects -
1) IAM permissions: Grant permissions to users or groups to view or create objects in a bucket.
2) Access Control Lists (ACLs): Use Google accounts to provide longer term access.
3) Signed URLs: Generate a URL to give time-limited read or write access to an object.
In addition to IAM and ACLs, the following tools are available to help you control access to your resources-
-Signed URLs (query string authentication).
-Signed Policy Documents.
-Firebase Security Rules.
-Public access prevention.
-Credential Access Boundaries.

Machine image is different from an instance
Yes, a machine image is different from an instance in the context of cloud computing. Here's an explanation to support this argument:
- Machine Image: A machine image, often referred to as an "image" or "AMI" (Amazon Machine Image) in the context of Amazon Web Services (AWS), is a pre-configured virtual machine template. It includes the operating system, software applications, and any other configurations needed for the virtual machine to function. Machine images serve as a blueprint for creating instances. They are essentially a snapshot or template used to replicate a specific system configuration.
- Instance: An instance is a running virtual machine created from a machine image. It represents a live and operational computing environment within the cloud infrastructure. Instances are dynamic and can be started, stopped, and terminated as needed. They use the resources defined by the machine image but are distinct entities with their own state, such as IP addresses, storage, and runtime data.
In summary, a machine image is a static, pre-configured template or snapshot used to create virtual machines, while an instance is a live, running virtual machine created from a machine image. The machine image is the blueprint, and the instance is the active instantiation of that blueprint in the cloud environment.


Simple Storage Service – S3:
(i) S3 - Overview: S3 is the popular web storage service offered by Amazon. Developers are provided with excellent benefits of simple, reliable, majorly-scalable, cost-effective and low-latency data storage platforms.
(ii) Buckets: A bucket is a type of data structure that is created in an Amazon Web Service (AWS) section to store multiple objects. These objects refer to the uploaded data in the format of documents, videos, images, etc. A bucket name is a globally unique name given at the time of bucket creation.
(iii) Objects: Objects can be stored in as many buckets as possible.
An object is composed of the following information:
• Key: Name given to the object.
• Version ID: Unique identification version number associated to the object when it is stored in a bucket.
• Value: Content stored in a bucket.
• Metadata: Set of name-value pairs supporting the information of the stored object.
• Sub-resources: Additional information related to the object.
• Access Control Information: Information related to the access control of the stored object (this information can be resource-based or user-based).
(iv) Logging: With respect to Amazon S3, Logging refers to the process of API call logging triggered from your AWS account to the specified bucket on S3. This call logging service is known as CloudTrail. Another logging service for server access is performed to track complete details of the requests made for accessing a bucket. The access log information is helpful in access audits and security.
(v) ACL: Access control lists (ACLs) refers to the resource-based access policy. It can be used to control access of your stored objects and the buckets. ACLs allow you to permit basic read/write operations for other AWS accounts. However, managing permissions using ACLs has certain limits.
(vi) Signed URL: A signed URL contains extra information. For instance, mentioned expiration time and date allows you to monitor your content at a more high level. A signed URL can be created using either a custom policy or a canned policy.
(vii) S3 Applications:
• Data Storage
• Data Transfer (Download/Upload)
• Files Back Up (Computer and online)
• Website hosting

Elastic Cloud Compute – EC2:
Elastic cloud compute refers to the processing of a computing system in which the machines are highly elastic or adaptable to accommodate the workload changes by allocating or de- allocating the available resources at each point in time to meet the active request in the cloud, in the best possible manner.
Amazon offers EC2 as a web service tool to provide high-end computation benefits to the developers:
• The EC2 interface provides minimized friction in the configuration capabilities.
• You can completely control your computing resources.
• Time is greatly minimized while starting new server instances.
• The elasticity feature allows you to effortlessly scale up or down your computing capacity as per the changing computing needs.
• EC2 is a cost-effective measure that makes you avoid the investment in hardware.
• The development and deployment of applications become fast and efficient.
• EC2 makes you work hassle-free without bothering the need to predict network traffic, app popularity spikes, etc.


Computer Network:
A computer network is a group of computers, two or more, connected to each other through a wire or a cable or even wireless.

Advantages of Network:
- It is easy to share data and files between users.
- Sharing resources, such as printers and fax machines helps reduce the cost and saves money.
- Communication has become a lot easier as network users can communicate with each other using e-mails and messengers.
- You get the freedom to remotely access your data on the network from anywhere.

Disadvantages of Network:
- There is always a chance of the files getting corrupted due to viruses, which can spread over a network.
- Hacking is also a major concern when it comes to the safety of data and information lying on the network. The network users are always advised to use security measures, such as Firewall on their computers.
- As easy as it seems from outside, the management of network cables and wires is equally complicated. It requires training and in case of organizations, they need to hire a set of trained staff and network managers for this job.

Types of Networking:
Networks are classified on the basis of scale. The area a network covers determines the type of network it is.
Originally, there were only two types of networks:
1. LAN (Local Area Network), and
2. WAN (Wide Area Network)
But, over the years, other types of networks have evolved, such as:
• VPN (Virtual Private Network)
• MAN (Metropolitan Area Network)
• CAN (Campus Area Network)
• PAN (Personal Area Network)
• DAN (Diverse Alert Network)
• EPN (Electronic Payments Network)

Local Area Network (LAN):
Definition: A LAN is a network that is limited to a small geographic area, such as a single building or a campus.
Examples: Office network, school network.
Characteristics:
High data transfer rates.
Limited geographical scope.
Private ownership and control.
Common communication protocols.
Advantages:
Fast data transfer within the network.
Easy resource sharing.
Cost-effective to set up and maintain.
High level of security and control.
Disadvantages:
Limited geographical coverage.
Potential for network congestion with increased users.
Requires additional hardware for expansion.
Relatively higher installation costs for high-speed LANs.

Wide Area Network (WAN):
Definition: A WAN covers a broad geographical area, often connecting LANs across cities or even countries.
Examples: Internet, global corporate networks.
Characteristics:
Extensive geographical coverage.
Relies on public and private telecommunication networks.
Slower data transfer rates compared to LANs.
Interconnected LANs.
Advantages:
Enables communication over long distances.
Centralized data management.
Shared resources among geographically dispersed locations.
Wide accessibility.
Disadvantages:
Slower data transfer rates than LANs.
Higher infrastructure costs.
Vulnerable to security threats.
Dependency on telecommunication providers.

Virtual Private Network (VPN):
Definition: A VPN is a secure network that uses the public internet to connect remote users or networks while maintaining privacy.
Examples: Remote work connections, business branch offices.
Characteristics:
Secure data encryption.
Cost-effective compared to private networks.
Enables remote access securely.
Supports various network protocols.
Advantages:
Secure communication over the internet.
Cost-effective alternative to private networks.
Remote access to the organization's network.
Scalability and flexibility.
Disadvantages:
Requires internet connectivity.
Performance may be affected by internet speed.
Potential security risks if not properly configured.
Dependence on third-party VPN providers.

Metropolitan Area Network (MAN):
Definition: A MAN spans a larger geographical area than a LAN but is smaller than a WAN, typically covering a city.
Examples: City-wide Wi-Fi networks, cable television networks.
Characteristics:
Geographical coverage between LAN and WAN.
High data transfer rates.
Used for connecting multiple LANs within a city.
Advantages:
Faster data transfer rates than WANs.
Suitable for connecting multiple LANs within a city.
Allows efficient resource sharing.
Lower infrastructure costs compared to WANs.
Disadvantages:
Limited coverage compared to WANs.
Requires significant initial setup costs.
Vulnerable to security threats.
Dependent on local infrastructure.

Campus Area Network (CAN):
Definition: A CAN connects multiple LANs within a specific geographic area, such as a university campus.
Examples: University campus network, corporate campuses.
Characteristics:
Intermediate geographical coverage between LAN and MAN.
High data transfer rates.
Typically owned and operated by a single organization.
Advantages:
Efficient communication within a specific campus.
Centralized resource management.
High-speed data transfer.
Lower infrastructure costs compared to WANs.
Disadvantages:
Limited coverage compared to WANs.
Initial setup costs can be high.
Requires maintenance and management.
Vulnerable to security threats.

Personal Area Network (PAN):
Definition: A PAN is a small network for personal devices, typically within the range of an individual person, such as smartphones and laptops.
Examples: Bluetooth-connected devices, wireless earbuds.
Characteristics:
Very limited geographical coverage.
Connects personal devices.
Often wireless technologies like Bluetooth.
Supports low-power communication.
Advantages:
Enables easy connectivity for personal devices.
Low power consumption.
Convenient for short-range communication.
Wireless and portable.
Disadvantages:
Limited coverage range.
Restricted to personal device connections.
Limited data transfer rates.
Susceptible to interference.

Diverse Alert Network (DAN):
Definition: A DAN is designed for emergency alert and notification systems, providing timely and widespread dissemination of critical information.
Examples: Emergency broadcast systems, public safety networks.
Characteristics:
Specialized for emergency alerts.
Wide coverage for public safety.
Supports various communication channels.
Quick and reliable information dissemination.
Advantages:
Rapid dissemination of critical information.
Wide coverage for public safety.
Redundant communication channels.
Real-time alerts for emergencies.
Disadvantages:
Limited to emergency alert purposes.
Dependence on reliable infrastructure.
Possibility of false alarms.
Requires coordination among authorities.

Electronic Payments Network (EPN):
Definition: An EPN facilitates electronic financial transactions, including online banking and digital payment systems.
Examples: Online banking networks, digital payment platforms.
Characteristics:
Secure and encrypted communication.
Facilitates electronic transactions.
Involves financial institutions and payment service providers.
High reliability and security standards.
Advantages:
Convenient and efficient electronic transactions.
Secure financial data transmission.
Facilitates online banking and digital payments.
Integration with financial institutions.
Disadvantages:
Vulnerability to cyber threats.
Dependence on secure encryption methods.
Regulatory compliance challenges.
Potential for fraud and unauthorized access.

Wireless communication setup:
Wireless communication is a type of communication that transmits signals without using a physical medium. Wireless communication can use electromagnetic waves, such as radio and microwaves, or light pulses.
Different types of wireless communication include: 
1.Wireless Local Area Network (WLAN): A group of devices that form a network based on radio transmissions. A Wi-Fi network is a type of WLAN. Wireless Local Area Networks (WLAN) are wireless networks that use radio waves, not Bluetooth technology like WPANs. There is usually at least one cable that is the access point for internet access, such as a wired internet connection going into a router, which then broadcasts the wireless signal to other devices.
Definition: WLAN is a type of local area network that uses wireless communication to connect devices within a limited geographic area, such as a building, campus, or home.
Examples: Wi-Fi networks in homes, offices, cafes, and public spaces.
Characteristics:
Wireless connectivity using radio waves.
High data transfer rates for short distances.
Commonly used for internet access and local resource sharing.
Typically employs Wi-Fi technology.
2.Personal Area Network (PAN):
Definition: PAN is a small network designed for personal devices within the range of an individual person, often using wireless technologies.
Examples: Bluetooth-connected devices, such as smartphones, laptops, and wireless earbuds.
Characteristics:
Very limited geographical coverage.
Wireless connectivity for personal devices.
Supports low-power communication.
Commonly used for short-range data exchange.
3.Wireless Metropolitan Area Network (WMAN):
Definition: WMAN is a network that covers a metropolitan area and uses wireless communication technologies.
Examples: City-wide Wi-Fi networks, wireless broadband networks.
Characteristics:
Larger geographical coverage than WLAN.
Supports high data transfer rates.
Used to provide wireless internet access across cities.
Utilizes technologies like WiMAX.
4.Wireless Wide Area Network (WWAN):
Definition: WWAN covers a wide geographical area, often on a national or global scale, using wireless communication.
Examples: Mobile cellular networks (3G, 4G, 5G), satellite communication.
Characteristics:
Broad coverage area, potentially global.
Utilizes mobile network infrastructure.
Supports mobility for devices across large distances.
Commonly used for mobile communication and internet access.


IP address:
An IP (Internet Protocol) address is a unique string of numbers separated with periods. It represents a device uniquely on the Internet or on your company's intranet that try to use the IP in order to communicate on a network. The IP addresses for networks on the Internet are allocated by the InterNIC (Internet Network Information Center).
When we have a valid Internet connection (which means you should have a registered domain and a permanent link to access the Internet, a dial-up connection is not sufficient), then our computer connected to the Internet is provided a valid network address by the InterNIC.
It is a numerical label assigned to the devices connected to a computer network that uses the IP for communication.
IP address acts as an identifier for a specific machine on a particular network. It also helps you to develop a virtual connection between a destination and a source. The IP address is also called IP number or internet address. It helps you to specify the technical format of the addressing and packets scheme. Most networks combine TCP with IP.
IP Address is divided into two parts:
-Prefix: The prefix part of IP address identifies the physical network to which the computer is attached. Prefix is also known as a network address.
-Suffix: The suffix part identifies the individual computer on the network. The suffix is also called the host address.

Classful Addressing:
Classful addressing is a network addressing the Internet’s architecture from 1981 till Classless Inter-Domain Routing was introduced in 1993.
This addressing method divides the IP address into five separate classes based on four address bits.
Here, classes A, B, C offers addresses for networks of three distinct network sizes. Class D is only used for multicast, and class E reserved exclusively for experimental purposes.

Limitations of classful IP addressing:
Here are the drawbacks/ cons of the classful IP addressing method:
-Risk of running out of address space soon.
-Class boundaries did not encourage efficient allocation of address space.

Classless Inter-Domain Routing (CIDR): 
CIDR or Class Inter-Domain Routing was introduced in 1993 to replace classful addressing. It allows the user to use VLSM or Variable Length Subnet Masks. 
CIDR notation: 
In CIDR subnet masks are denoted by /X. For example a subnet of 255.255.255.0 would be denoted by /24. To work a subnet mask in CIDR, we have to first convert each octet into its respective binary value. For example, if the subnet is of 255.255.255.0. then : 
First Octet –
255 has 8 binary 1's when converted to binary 
Second Octet –
255 has 8 binary 1's when converted to binary 
Third Octet –
255 has 8 binary 1's when converted to binary 
Fourth Octet –
0 has 0 binary 1's when converted to binary 
Therefore, in total there are 24 binary 1’s, so the subnet mask is /24.
While creating a network in CIDR, a person has to make sure that the masks are contiguous, i.e. a subnet mask like 10111111.X.X.X can’t exist. 
With CIDR, we can create Variable Length Subnet Masks, leading to less wastage of IP addresses. It is not necessary that the divider between the network and the host portions is at an octet boundary. For example, in CIDR a subnet mask like 255.224.0.0 or 11111111.11100000.00000000.00000000 can exist.

CIDR Notation:
An IP address range in CIDR is specified using a combination of an IP address and its associated network mask. A CIDR IP address is similar to an IP address. The only difference is that it ends with a slash followed by a number, called the IP network prefix.
For example: 194.300.0.0/11.
The IP network prefix specifies how many addresses are covered by the CIDR address, with lower numbers covering more addresses. An IP network prefix of /11, for example, can be used to address 1,048,576 former Class C addresses.

Classful Addressing vs Classless Addressing:
1. In Classful addressing IP addresses are allocated according to the classes- A to E.	Classless addressing came to replace the classful addressing and to handle the issue of rapid exhaustion of IP addresses.
2. Classful Addressing	is less practical.	Classless Addressing is more practical.
3. In Classful addressing, The changes in the Network ID and Host ID depend on the class. In Classless addressing, There is no such restriction of class in classless addressing.
4. Classful addressing does not support the Variable Length Subnet Mask (VLSM). Classless addressing supports the Variable Length Subnet Mask (VLSM).
5. Classful addressing requires more bandwidth. As a result, it becomes slower and more expensive as compared to classless addressing. Classless addressing requires less bandwidth. Thus, fast and less expensive as compared to classful addressing.
6. It does not support Classless Inter-Domain Routing (CIDR). Classless addressing supports Classless Inter-Domain Routing (CIDR).
7. In Classful addressing, Regular or periodic updates. In Classless addressing, Triggered Updates
8. In Classful addressing, Troubleshooting and problem detection are easy than classless addressing because of the division of network, host and subnet parts in the address. In Classless addressing, It is not as easy compared to classful addressing.
9. Division of Address in Classful - Network, Host, Subnet. Division of Address in Classless - Host, Subnet

Classless Routing vs Classful Routing:
In Classless Routing, Protocols include the subnet masks in the updates.In Classful Routing, Protocols do not include the subnet mask in the updates and cannot support variable-length subnet mask.

IP Classes:
Depending on the size of the network, IP-based networks are divided into the following classes.
-Class A: Class A networks are mega-monster networks that contain addresses for about 16 million network interfaces. Class A networks have their network addresses from 1.0.0.0 to 127.255.255.255, with the zeros being replaced by node addresses. An example of a Class A address is 102.168.212.226. Here, “102” helps you identify the network and 168.212.226 identify the host. Class A addresses 127.0.0.0 to 127.255.255.255 cannot be used and is reserved for loopback and diagnostic functions.
-Class B: Class B networks are smaller networks in comparison to Class A networks. They can have only about 65,000 nodes. Network addresses for these ranges from 128.0.0.0 to 191.255.255.255, with the last two zeros being replaced by the node addresses. An example of Class B IP address is 168.212.226.204, where *168 212* identifies the network and *226.204* helps you identify the Hut network host.
-Class C: These are the baby networks that can have a maximum of 254 nodes. The network IP addresses for these range from 192.0.0.0 to 223.255.255.255. Example for a Class C IP address: 192.168.178.1
-Class D: Class D addresses are only used for multicasting applications. Class D is never used for regular networking operations. This class addresses the first three bits set to “1” and their fourth bit set to use for “0”. Class D addresses are 32-bit network addresses. All the values within the range are used to identify multicast groups uniquely. Therefore, there is no requirement to extract the host address from the IP address, so Class D does not have any subnet mask. Example for a Class D IP address: 227.21.6.173
-Class E: Class E IP address is defined by including the starting four network address bits as 1, which allows you two to incorporate addresses from 240.0.0.0 to 255.255.255.255. However, E class is reserved, and its usage is never defined. Therefore, many network implementations discard these addresses as undefined or illegal. Example for a Class E IP address: 243.164.89.28

Ranges and no. of nodes of IP classes:
-Class A: 1.0.0.0 to 127.255.255.255. 16 million nodes.
-Class B: 128.0.0.0 to 191.255.255.255. 65,000 nodes
-Class C: 192.0.0.0 to 223.255.255.255. 254 nodes
-Class D: 224.0.0.0 to 239.255.255.255. Reserved for multicasting.
-Class E: 240.0.0.0 to 255.255.255.255. Reserved for research and development purposes.

IP address is basically divided into two parts: 
If X1. X2. X3. X4 is a IP address then-
1. [X1. X2. X3] is the Network ID: It is the part of the left-hand IP address that identifies the specific network where the device is located. In the normal home network, where the device has an IP address 192.168.1.32, the 192.168.1 part of the address will be the network ID. It is customary to fill in the last part that is not zero, so we can say that the device’s network ID is 192.168.1.0.
2. [X4] is the Host ID: The host ID is part of the IP address that was not taken by the network ID. Identifies a specific device (in the TCP / IP world, we call devices “host”) in that network. Continuing with our example of the IP address 192.168.1.32, the host ID will be 32- the unique host ID on the 192.168.1.0 network.

Version of IP address:
Currently there are 2 versions of IP addresses are in use i.e IPV4 and IPV6
- IPV4 (Internet Protocol Version 4): It is the first version of Internet Protocol address. The address size of IPV4 is 32 bit number. In this Internet Protocol Security (IPSec) with respect to network security is optional. It is having 4,294,967,296 number of address still we are seeing a shortage in network addresses as the use of network & virtual devices are increasing rapidly.
- IPV6 (Internet Protocol Version 6): It is the recent version of Internet Protocol address. The address size of IPV6 is 128 bit number. In this Internet Protocol Security (IPSec) with respect to network security is mandatory. It allows 3.4 x 10^38 unique IP addresses which seems to be more than sufficient to support trillions of internet devices present now or coming in future.

IPv4 vs IPv6:
1.IPv4 has a 32-bit address length. IPv6 has a 128-bit address length.
2.IPv4 Supports Manual and DHCP address configuration. IPv6 supports Auto and renumbering address configuration.
3.In IPv4 end to end, connection integrity is Unachievable. In IPv6 end-to-end, connection integrity is Achievable.
4.IPv4 can generate 4.29×109 address space. The address space of IPv6 is quite large it can produce 3.4×1038 address space.
5.The Security feature is dependent on the application. IPSEC is an inbuilt security feature in the IPv6 protocol.
6.Address representation of IPv4 is in decimal Address. Representation of IPv6 is in hexadecimal.
7.Fragmentation performed by Sender and forwarding routers. In IPv6 fragmentation is performed only by the sender.
8.In IPv4 Packet flow identification is not available. In IPv6 packet flow identification are Available and uses the flow label field in the header.
9.In IPv4 checksum field is available. In IPv6 checksum field is not available.
10.It has a broadcast Message Transmission Scheme. In IPv6 multicast and anycast message transmission scheme is available.
11.In IPv4 Encryption and Authentication facility not provided. In IPv6 Encryption and Authentication are provided.
12. IPv4 has a header of 20-60 bytes. IPv6 has a header of 40 bytes fixed. 
13. IPv4 can be converted to IPv6. Not all IPv6 can be converted to IPv4
14. IPv4 consists of 4 fields which are separated by addresses dot (.). IPv6 consists of 8 fields, which are separated by a colon (:)
15. IPv4’s IP addresses are divided into five different classes. Class A , Class B, Class C, Class D , Class E.	IPv6 does not have any classes of the IP address.
16. IPv4 supports VLSM(Variable Length subnet mask). IPv6 does not support VLSM.
17. Example of IPv4: 66.94.29.13. Example of IPv6: 2001:0000:3238:DFE1:0063:0000:0000:FEFB

IP Address Types:
There are 4 types of IP Addresses- Public, Private, Fixed, and Dynamic. Among them, public and private addresses are derived from their local network location, which should be used within the network while public IP is used offline.
- Public IP address: A public IP address is an Internet Protocol address, encrypted by various servers/devices. That’s when you connect these devices with your internet connection. This is the same IP address we show on our homepage. So why the second page? Well, not all people speak the IP language. We want to make it as easy as possible for everyone to get the information they need. Some even call this their external IP address. A public Internet Protocol address is an Internet Protocol address accessed over the Internet. Like the postal address used to deliver mail to your home, the public Internet Protocol address is a different international Internet Protocol address assigned to a computer device. The web server, email server, and any server device that has direct access to the Internet are those who will enter the public Internet Protocol address. Internet Address Protocol is unique worldwide and is only supplied with a unique device.
- Private IP address: Everything that connects to your Internet network has a private IP address. This includes computers, smartphones, and tablets but also any Bluetooth-enabled devices such as speakers, printers, or smart TVs. With the growing internet of things, the number of private IP addresses you have at home is likely to increase. Your router needs a way to identify these things separately, and most things need a way to get to know each other. Therefore, your router generates private IP addresses that are unique identifiers for each device that separates the network.
- Static IP Address: A static IP address is an invalid IP address. Conversely, a dynamic IP address will be provided by the Dynamic Host Configuration Protocol (DHCP) server, which can change. The Static IP address does not change but can be changed as part of normal network management. Static IP addresses are incompatible, given once, remain the same over the years. This type of IP also helps you get more information about the device.
- Dynamic IP address: It means constant change. A dynamic IP address changes from time to time and is not always the same. If you have a live cable or DSL service, you may have a strong IP address. Internet Service Providers provide customers with dynamic IP addresses because they are too expensive. Instead of one permanent IP address, your IP address is taken out of the address pool and assigned to you. After a few days, weeks, or sometimes even months, that number is returned to the lake and given a new number. Most ISPs will not provide a static IP address to customers who live there and when they do, they are usually more expensive. Dynamic IP addresses are annoying, but with the right software, you can navigate easily and for free. 

Subnetting:
A subnet, which is a short form of subnetwork, is an identifiable, logical part of an IP network. A subnet mask is used to divide a given network address into two or more subnets. When a router uses the subnet, it does not need to read the entire 32-bit address. It focuses on the subnet mask and the bits selected by the subnet mask, which saves time. The default subnet masks for class A network is 255.0.0.0, for class B is 255.255.0.0 and for class C is 255.255.255.0, which signifies a network without subnets.
The process of dividing a network into smaller network sections is called subnetting. Subnetting can be useful for isolating groups of hosts together so that they can be dealt easily. The process of subnetting allows the administrator to break down the network of Class A, Class, B, or Class C into smaller, manageable portions. If the breaking down is not sufficient, then the subnets can further be broken down into sub-subnets.
When we break down the network into subnets, we can achieve the mentioned benefits:
-We can experience reduced network traffic since the broadcast volume has been reduced.
-We can access a work network from your home, without actually opening the complete network.

Need for Subnetting:
Subnetting is necessary to efficiently manage and organize IP networks. It allows for the division of a network into smaller, logical subnets, providing benefits such as reduced network traffic by minimizing broadcast volume. Subnetting also facilitates easier administration by isolating groups of hosts, and it enables more efficient use of IP addresses, aiding in the optimal allocation of resources. Additionally, subnetting allows for improved security and accessibility, as specific subnets can be isolated or accessed as needed.
- Efficient network management through logical segmentation using subnets.
- Conservation of IP addresses by breaking down larger networks into smaller, manageable portions.
- Reduction of network traffic and broadcast volume, leading to improved performance.
- Enhanced security by isolating groups of hosts within specific subnets.
- Facilitates remote access and connectivity to specific network segments rather than the entire network.

Advantages of Subnetting:
-Subnetting is used to decrease the presence of Internet Protocol (IP) range.
-Subnets helps in stopping the devices or gadgets from occupying the whole network, only allowing the hosts to control which kind of user can have access to the important information. Simply, we can tell that network is safe just because of the subnetting concept.
-Subnetting concept increases the performance of the total network by deleting the repeated traffic causing errors.
-We can convert the whole big network into smaller networks by using the concept of subnetting as discussed earlier.

Disadvantages of Subnetting:
- If the number of subnets increases, then the number of routers must also increase along with the subnet increase number. This happens because each subnet has its own subnet mask, broadcast address and network address.
- As told earlier, if we create many subnets many IP Addresses are wasted because of the wastage of Host ID Bits
- The cost of the entire network is increased by subnetting, which calls for the acquisition of pricey internal routers, switches, hubs, and bridges, among other things.
- The complexity of the network is increased through subnetting. The subnet network must be managed by a skilled network administrator.


IPv4 Header:
An IP header is bits of information attached to each data packet that is transported in the computer network. This information usually includes addressing and routing details which makes it possible to reassemble the packets and have the original data at the destination.

Fields of IPv4 Header:
The IPv4 header contains 13 fields. These fields are Version, Internet Header Length, Type of Service, Total Length, Identification, Flags, Fragment offset, Time-to-Live, Protocol, Header Checksum, Source address, Destination address, and Options. The following image shows how these fields are arranged in the IP header.
1.Version (4 bits): Indicates the IP protocol version; set to 4 for IPv4.
2.IHL (4 bits): Represents the length of the IPv4 header in 4-byte blocks.
3.Type of Service (8 bits): Specifies the desired service for packet delivery, including precedence, delay, throughput, reliability, and cost characteristics.
4.Total Length (16 bits): Indicates the total length of the packet (header + payload).
5.Identification (16 bits): Used for reassembling fragmented packets.
6.Flags (3 bits): Controls fragmentation; indicates whether a packet can be fragmented and if more fragments follow.
7.Fragment Offset (13 bits): Indicates the position of a fragment relative to the beginning of the payload.
8.Time-to-Live (8 bits): Specifies the maximum number of hops a packet can traverse before being discarded.
9.Protocol (8 bits): Identifies the upper-layer protocol handling the payload (e.g., UDP or TCP).
10.Header Checksum (16 bits): Provides a checksum for the header to verify integrity during transmission.
11.Source Address (32 bits): Stores the IPv4 address of the sending device.
12.Destination Address (32 bits): Stores the IPv4 address of the destination device.
13.Options (multiple of 32 bits): Stores IPv4 options, with padding options for non-4-byte-aligned lengths.

Some of the IP options:
Security: Specifies how secret the datagram is
Strict source routing: Gives the complete path to be followed
Loose source routing: Gives a list of routers not to be missed
Record route: Makes each router append its IP address
Timestamp: Makes each router append its address and timestamp

Host and Net Calculation:
Example-
IP Address: 192.168.1.0
Subnet Mask: 255.255.255.128 (or /25)
Steps:
-Class: Class C
-Binary: IP - 11000000.10101000.00000001.00000000; Mask - 11111111.11111111.11111111.10000000
-Network/Host Bits: First 25 bits for network, last 7 bits for hosts.
-Subnets: 2^2 = 4 subnets (with two additional subnet bits).
-Hosts: 2^7 - 2 = 126 hosts per subnet.
-Network Address: 192.168.1.0
-Broadcast Address: 192.168.1.127
-Usable Host Range: 192.168.1.1 to 192.168.1.126
-Subnetting: Repeat steps for each subnet.
-Subnetting Table: Organize information for each subnet.

Network Devices:
The components of networking are networking devices. Networking devices are mainly used for communication and, hence, are also known as communicating devices.
To connect computers and any other electronic devices together, network devices are used. These devices when connected can share data or files or resources like fax machines, printers, scanners, etc. Devices used to setup a LAN are the most common type of network devices used by the public. A LAN requires a hub, switch, or router.

Gateways:
A gateway is a network point or node that acts as an entrance to another network. With the help of gateway data can be sent back and forth and the communication is made possible between the Internet users.
The default Internet gateways are typically one of the two types:
• At home or small business networks, with a broadband router to share the Internet connection, the home router serves as the default gateway.
• At home or small business networks without a router, such as for residences with dial-up Internet access, a router at the ISP location serves as the default gateway.

Node:
A node is simply a physical place where the data stops for either transporting or reading or using.
• On the Internet, the node that is a stopping point can be a gateway or a host or end- point node.
• A computer that controls the traffic received by your Internet Service Provider (ISP) is a node.

Routers:
They are small electronic devices that join different computer networks together. Routers can use either wired or wireless connections.
These are hardware devices specially designed to receive, analyse and forward incoming data packets along the connected network. The other functions of routers can be to convert the packets to another network interface, drop them and perform other actions related to a network.
It contains a processor (CPU), several kinds of digital memory and input-output (I/O) interfaces.
A router commonly connects either two LANs or WANs or it may connect a LAN along with its ISP network. Routers are placed at gateways, which is the meeting point of two or more networks.

There are following types of Routers:
• Bridge router (Brouter) – It is networking device that serves as both a bridge and a router.
• Core router – It routes data within a network, but not between networks.
• Edge router – It routes data packets between one or more LANs.
• Virtual router – It is a backup router in a Virtual Router Redundancy Protocol (VRRP) setup. A virtual router is a software-based system that does everything a hardware router can do.
• Wireless router – A wireless router functions both as a router and a wireless access pointin wireless local area network (WLAN). It determines the next available network point towards which the data should be forwarded on the way to its destination.

Utilities:
In order to configure and analyse different aspects of computer networks, we have specially designed software utilities known as network utilities.
Some common network utilities are as follows:
-Ping: Tests network connectivity by sending a simple request and measuring response time.
-Traceroute/tracert/tracepath: Maps and displays the route packets take to reach a destination, revealing network path details.
-Ipconfig: Displays and configures network interface information on Windows systems.
-nslookup: Queries DNS to obtain domain name or IP address information.
-whois: Retrieves domain registration details and ownership information.
-netstat: Shows active network connections, listening ports, and routing tables.
-finger: Retrieves user information, including login name and idle time.
-port scan/nmap: Maps network services and identifies open ports on a target system for security assessment.

Key pairs:
The security levels are associated with a specific kind of credential or cryptography key. For instance, to acquire a high-level security, each and every node should have a valid and authentic public and private key pairs.
-Private Key: One of the two keys used in public-key encryption is a private key. The private key remains with the user and is kept secret. This private key is used by the user to encrypt outgoing messages and decrypt incoming messages. To maintain the security, we need to set the permissions for the private key in a way set so that the read and write access is reserved for the owner. This is more important if the key does not have a passphrase. In usage, a passphrase is similar to a password but is kept longer for added security.
-Public Key: Other than the private key, the other key used in a public-key encryption is a public key. The user bearing the private key releases a copy of the public key to allow anyone to use this key for decrypting messages received from the user and encrypting a message before sending it back to the user. Let us consider an example. A, the message initiator or sender, sends a message to B. The message sent by A is encrypted with B's public key, while B uses its private key to decrypt A's received message

Need of Using Key-pairs:
- Secure Communication: Enables encrypted messages for confidentiality.
- Authentication: Verifies user identity through private key possession.
- Data Integrity: Maintains message integrity during transmission.
- Non-Repudiation: Prevents users from denying their actions.
- Access Control: Manages permissions for data encryption and decryption.

Security Groups:
Security groups are a collection of users who have similar permissions and rights to access resources and perform certain system-related tasks in common. It provides a systematic approach to grant access to resources on your network.
We can use security groups to do the following:
- Assign user rights to security groups in Active Directory (AD).
- Assign permissions to security groups for resources.

Different types of security groups:
-Local groups: The security groups that are confined and specific to an individual computer fall under local groups. Although local computers are connected to a domain, they have separate user accounts that do not belong to the connected domain.
-Domain local groups: The security groups that are commonly used to assign permissions for granting access to resources fall under domain local groups. These permissions can be assigned only in the domain where the domain local group has been created. You can add members of different domains to the domain local group. Apart from that, it can also contain user accounts, global groups and universal groups from any domain.
-Global groups: This security group is the most commonly used type of group, by far. In maximum cases, a global group acts as a repository of AD user accounts. The global groups can be nested within each other by making a global group member of another global group. This can be done when both the groups exist within the same domain.

Amazon SDB:
Amazon SDB is a scalable and flexible non-relational data store that offloads the work of database administration. It provides the essential database functions of data indexing and querying in the cloud. SDB provides a simple interface that enables users to create and store multiple data sets. It also allows users to query their data and return the results. The service manages hardware and software maintenance, infrastructure provisioning, performance tuning and replication and indexing of data items. This enables users to focus on application development and just pay for the resources they actually consume storing data and issuing requests.
Amazon SDB also allows users to partition their workload across multiple domains, thereby enabling scalability. If your workload exceeds the storage and request quantity provided by a single domain, you can create additional domains. This will help you obtain higher throughput. For instance, if you use 10 domains for your data and execute 10 queries in parallel, your throughput will be much higher.
SDB’s flexibility allows users to change their data models on the fly. This means that users can add or remove attributes without breaking a schema. As a result, the changes to the application and business are reflected quickly without expensive refactoring or tedious schema updates.
Another benefit of Amazon SDB is that it offers automatic, geo-redundant replication. Every time a data item is stored, multiple replicas are created in various data centres within the selected region. This enables data durability and availability in the event of a data centre outage. SDB also automatically indexes data to facilitate efficient queries. In addition, it provides a simple API for storage and access.

Amazon SNS:
Amazon SNS is flexible, fast and a fully managed push notification service that allows users to send individual or bulk messages to any number of recipients. SNS makes it easy and cost- effective to send push notifications to email recipients, mobile device users and even to other distributed services.
SNS has a distributed publish-subscribe system. This means that messages are pushed to subscribers as and when they are sent by publishers. The service supports various end points such as sms, email, http and SQS.
With SNS, you can send push notifications to Google, Apple, Fire OS and Windows devices. With Baidu Cloud Push, you can send messages to Android devices in China.
SNS features the following capabilities:
• Users can create topics and publish messages to these topics.
• Subscribers to these topics will get messages pushed to them.
• Messages can be pushed over SMTP (email) or HTTP.
• Access control policies allow users to control subscribers to a topic.
• If message delivery fails, the SNS system retries sending it.

Amazon RDS:
Setting up and operating a relational database in the cloud is fairly simple and trouble-free with Amazon RDS. It provides resizable capacity and cost-efficient solutions to managing time-consuming database administration tasks. RDS allows users to choose from six familiar database engines - Oracle, Amazon Aurora, Microsoft SQL Server, MySQL, PostgreSQL and MariaDB. This means that applications, codes and tools that you are using with existing databases can be used with Amazon RDS. The service automatically patches the database software and backs up your database. It stores the backups for a user-defined retention period and enables point-in-time recovery. It is possible to scale the storage capacity of a database instance via a single API call.
Amazon RDS also enables replication. This works to enhance the availability and reliability for production workloads. In case of a failure, you can run mission-critical workloads with high availability and built-in automated failover from your primary database to the replicated secondary database. For read-heavy database workloads, it is possible to scale out beyond the capacity of a single database deployment with Amazon RDS for MySQL.

Programming:
Programming and controls are complementary to each other. Programming helps to find out the solution of any problem in any software. Amazon CloudFront acts as a content delivery network and helps to improve the downloading speed of the content. If no local file is available, CloudFront provides the copy of that file. It lowers the cost to deliver content online.
The available and supported programming languages that can be used with CloudFront include:
• Java: Developed by James Gosling at Sun Microsystems, this programming language was released in 1995. At present, it is owned and maintained by Oracle. Java is widely used to create software programs and Internet applications.
• Ruby: It is an open source object-oriented programming language which was first released in 1995. In this programming language, even the most basic data types, such as integers, have instance variables and methods.
• Python: It is an interactive and object-oriented programming language developed by Guido van Rossum and released in 1991.
• PHP: Developed by Rasmus Lerdorf and released in 1995, PHP is short for PHP: Hypertext Preprocessor. It is a server-side interpreted scripting language that is widely used for creating web pages which work effectively with databases.
• Windows: This is an operating environment created by Microsoft. Users can navigate through menus, buttons, tabs, dialog boxes and icons and this eliminates the need to memorise commands for the command line.
• .Net: This is a programming model that enables software developers to bundle a collection of software in one package. This, in turn, facilitates rapid application development.
Amazon Web Services (AWS) has libraries and resources that are available for these languages. These libraries and resources provide some basic functions such as accepting requests, checking the authenticity of the user and handling errors.

Control Structures:
Control structures enable users to determine the order in which program statements are executed. A language is made up of statements that are connected together by simple and powerful control structures. If statements are written for a logical use, the starting point is known as the entry point and the last statement is written as the exit point.
These control structures allow users to do two things:
1. Skip some statements while executing others.
2. Repeat one or more statements if some condition is true.

Types of Control Structures:
The three basic types of control structures are sequential, iteration and selection. They can be used together in any way to solve a specified problem. Implementing these control structures influences the flow of control through any given function. Within these control structures, minor variations may arise from the interaction of or the restrictions on the given control capabilities.
1) Sequential: This is the default control structure where statements are executed line by line in the order in which they occur. A sequence of statements is executed depending on whether the condition is true or false. Simply put, this means that the program chooses between two or more alternative paths. Condition refers to any value or expression that returns a Boolean value, meaning either true or false. 
2) Iteration: This is used for repeating or looping a piece of code multiple times in a row. The iteration or repetition structure repeatedly executes a series of statements as long as the condition is true. The condition may be open-ended or predefined. “While”, “do/while” and “for” loops are the three types of iteration statements. A loop can either be counter controlled or event controlled. A counter-controlled loop executes the statements a predetermined number of times while an event-controlled loop executes a sequence of statements till the event occurs.
3) Selection: This is used for decisions or branching where it is necessary to choose between two or more alternative paths. The three main types of selection statements are “if”, “if/else” and “switch” statements. The “if” statement is the most basic and common selection statement. The “if” and “if/else” statements can be nested. “Switch” statements are ideally used when there are multiple cases from which a selection can be made. In the case of selection control, if there is a problem, there can be two opinions - the problem may be either true or false. If it is true, there are some steps to follow but if the problem is false, it follows a different route. In this case, the selection box works to determine whether to proceed or not.

Apache Instances in EC2:
We can install the Apache web server on your Amazon Linux instance with PHP and MySQL support. This server can then be used to host a static website. You can also deploy a dynamic PHP application that reads and writes information to a database.
Setting up an Ubuntu Amazon EC2 instance with Apache, PHP and MySQL (also known as LAMP) to host a simple dynamic web site involves the following steps:
1. Creating a Security Group for web access.
2. Creating a LAMP-friendly instance.
3. Creating an Elastic IP.
4. Installing LAMP via SSH.

To configure Apache, we need to follow the steps:
1. Create and test an Amazon EFS file system.
2. Create a load balancer in your Amazon VPC.
3. Create an Auto Scaling group with two EC2 instances.
4. Create a sample page.
